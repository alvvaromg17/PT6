---
title: "Práctica Tema 6"
author: "Álvaro Miranda García"
date: "2023-03-27"
output:
  html_document: default
---



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



__1. Instale y cargue las siguientes librerías: “MASS”, “caret”, “stat”, “olsrr”, “kable”, “kableExtra”, “knitr” y “rmarkdown"__
  
Han sido instaladas.

__2. Cree 2 variables almacenadas como vector: “y_cuentas” y “x_distancia” a partir de los siguientes valores numéricos:__
```{r}
y_cuentas = c(110,2,6,98,40,94,31,5,8,10)
x_distancia = c(1.1,100.2,90.3,5.4,57.5,6.6,34.7,65.8,57.9,86.1)
```



__3. Verifique el supuesto de linealidad de la variable explicativa incluyendo un contraste de hipótesis.__

Verificar el supuesto de linealidad de la variable explicativa incluyendo un contraste de hipótesis implica comprobar si existe una relación lineal significativa entre la variable independiente (explicativa) y la variable dependiente. Esto se puede hacer visualizando la relación entre las dos variables mediante un gráfico de dispersión y verificando si parece que existe una tendencia lineal.

Además, también se puede realizar un análisis estadístico formal para determinar si hay una relación lineal significativa entre las dos variables. Esto se puede hacer mediante un contraste de hipótesis en el que la hipótesis nula es que no hay relación lineal significativa entre las dos variables, mientras que la hipótesis alternativa es que hay una relación lineal significativa.

En el análisis estadístico, se utiliza el coeficiente de correlación para medir la fuerza y la dirección de la relación entre las dos variables. Si el coeficiente de correlación es cercano a 1, entonces hay una relación lineal positiva fuerte entre las dos variables, mientras que si es cercano a -1, hay una relación lineal negativa fuerte. Si el coeficiente de correlación es cercano a 0, entonces no hay una relación lineal significativa entre las dos variables.

```{r}
plot(x_distancia, y_cuentas, xlab = "Distancia", ylab = "Cuentas")

cor(x_distancia, y_cuentas)
```


__4. Verifique el supuesto de normalidad de la variable explicativa mediante su visualización en un histograma y un test de normalidad.__

El supuesto de normalidad es un supuesto básico en muchos modelos estadísticos que implica que los errores o residuos de un modelo siguen una distribución normal (también conocida como distribución gaussiana).

En otras palabras, este supuesto establece que las observaciones en una muestra aleatoria se distribuyen normalmente alrededor de la media poblacional. Si los datos no se ajustan a una distribución normal, los resultados de un modelo pueden ser inexactos o incorrectos.

La normalidad se puede verificar visualmente mediante un histograma o un gráfico Q-Q (cuantil-cuantil), donde se compara la distribución de los datos con una distribución normal teórica. Además, se puede utilizar un test de normalidad, como el test de Shapiro-Wilk, que aplicamos a continuación, o el test de Kolmogorov-Smirnov; para cuantificar y determinar si una muestra de datos se ajusta a una distribución normal.

```{r}
hist(x_distancia)
shapiro.test(x_distancia)
```


__5. Multiplique las variable de respuesta por la variable explicativa. Llama al objeto “xy”.__
```{r}

xy = y_cuentas * x_distancia
xy


```


__6. Eleve al cuadrado la variable explicativa. Llama al objeto “x_cuadrado".__
```{r}
x_cuadrado <- x_distancia^2

x_cuadrado

```

__7. A continuación, almacena las variables: “y_cuentas”, “x_distancia”, “xy” y “x_cuadrado” en un data frame llamado “tabla_datos”__

```{r}

tabla_datos <- data.frame(y_cuentas, x_distancia, xy, x_cuadrado)



```


__8. Visualice el objeto “tabla_datos” en una tabla en la consola a través de alguna de las funciones ofrecidas por la librería “kableExtra”.__
```{r}

library(kableExtra)
kable(tabla_datos, caption = "Tabla de Datos") %>%
  kable_styling(full_width = F)
  
```

Añado, además, otra forma de hacer una tabla interesante e interactiva (aunque no en pdf), utilizando la librería DT y la función datatable (df).

```{r}
library (DT)
datatable (tabla_datos)
```


__9. Realice el sumatorio de los valores almacenados en las 4 columnas del data frame “tabla_datos”.__

```{r}

#primer sumatorio

sum1 = sum (tabla_datos$y_cuentas)

#segundo sumatorio

sum2 =  (tabla_datos$x_distancia)

#tercer sumatorio

sum3 = (tabla_datos$xy)

#cuarto sumatorio

sum4 = (tabla_datos$x_cuadrado)


```


__10. Añada el sumatorio de las 4 columnas como un último registro en el data frame “tabla_datos” de modo que tengamos en un solo objeto los valores junto con el sumatorio.__
```{r}



sum.y_cuentas <- sum(tabla_datos$y_cuentas)
sum.x_distancia <- sum(tabla_datos$x_distancia)
sum.xy <- sum(tabla_datos$xy)
sum.x_cuadrado <- sum(tabla_datos$x_cuadrado)

row.sums <- c(sum.y_cuentas,sum.x_distancia,sum.xy,sum.x_cuadrado)

tabla_datos_final <- rbind(tabla_datos, row.sums)

tabla_datos_final


```

$$\begin{equation}
\sigma^2 = \frac{\sum\limits_{i=1}^{n}(y_i – \bar{y})^2} {n – 1}
\end{equation}$$



__11. Calcule la recta de regresión por el método de mínimos cuadrados (ordinario) a través de los datos incluidos en el data frame “tabla_datos”__

La recta de regresión mediante el método de mínimos cuadrados (ordinario) es una línea recta que se ajusta a un conjunto de datos para predecir o modelar la relación entre una variable independiente (explicativa) y una variable dependiente.

El método de mínimos cuadrados ordinarios (MCO) busca la recta que minimiza la suma de los cuadrados de las diferencias entre los valores observados y los valores predichos por la recta. La recta de regresión obtenida por este método se puede utilizar para hacer predicciones de valores de la variable dependiente para valores específicos de la variable independiente.

La recta de regresión se puede expresar en forma de ecuación: Y = a + bX, donde Y es la variable dependiente, X es la variable independiente, a es la intersección en el eje Y (valor de Y cuando X es 0) y b es la pendiente de la recta (cambio en Y dividido por el cambio en X). La pendiente de la recta indica el cambio en la variable dependiente por cada unidad de cambio en la variable independiente.


```{r}
modelo <- lm(y_cuentas ~ x_distancia)

```



__12. Visualice en un gráfico de dispersión la recta de regresión, nube de puntos. Indique en el título la ecuación resultante y edite los nombre de los ejes.__

```{r}

plot(x_distancia, y_cuentas, main = paste("Y =", round(modelo$coefficients[2],4),"* X + ", round(modelo$coefficients[1],4)), xlab = "Distancia", ylab = "Cuentas")
abline(modelo)

```


__13. Calcule los residuos, residuos estandarizados y residuos estudentizados del modelo recién ajustado.__

Los residuos son la diferencia entre los valores observados de la variable dependiente y los valores estimados por el modelo de regresión. Es decir, los residuos son las diferencias entre los valores reales y los valores que el modelo de regresión predice para cada observación.

Los residuos estandarizados son los residuos divididos por la desviación estándar de los residuos. Esto se utiliza para comparar la magnitud de los residuos en diferentes modelos de regresión. 

Los residuos estudentizados, por otro lado, son una medida de los residuos que tienen en cuenta la varianza de los errores. Los residuos estudentizados se calculan dividiendo el residuo por la desviación estándar estimada de los errores. Los residuos estudentizados se utilizan para detectar valores atípicos o puntos influyentes en un modelo de regresión.

```{r}
residuos <- resid(modelo)
residuos

residuos_estandarizados <- rstandard(modelo)
residuos_estandarizados

residuos_estudentizados <- rstudent(modelo)
residuos_estudentizados
```

__14. Calcula el pronóstico o estimación del modelo para una observación que registra una distancia de 6.6km con respecto a la mina.__
```{r}
predict(modelo, newdata = data.frame(x_distancia = 6.6))

```


$$\begin{equation}
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i
\end{equation}$$


__15. Genera dos conjuntos aleatorios de datos: “entrenamiento” y “validación”__
```{r}
set.seed(12345)
entrenamiento <- sample(1:nrow(tabla_datos), 0.7*nrow(tabla_datos))
validacion <- setdiff(1:nrow(tabla_datos), entrenamiento)

```


__16. Ajusta nuevamente el modelo con el conjunto de “entrenamiento”.__
```{r}
modelo_entrenamiento <- lm(y_cuentas ~ x_distancia, data = tabla_datos, subset = entrenamiento)

```


__17. Interprete el valor asociado a los coeficientes de regresión y a R2. ¿Qué significan los asteriscos inmediatamente a la derecha de los valores arrojados tras ajustar el modelo?__

```{r}
summary(modelo_entrenamiento)

```
Los asteriscos significan que el coeficiente de regresión es significativo estadísticamente para un nivel de significación del 5%. 

__18. ¿Cómo se ha realizado el cálculo para los grados de libertad del modelo?__

Los grados de libertad se calculan restando el número de observaciones menos el número de parámetros estimados. En este caso, es 8.


__19. Especifique el total de varianza explicada y no explicada por el modelo.__


```{r}

#Varianza explicada:
ssr = sum(residuos^2) 

#Varianza no explicada:
sse = sum((y_cuentas - predict(modelo))^2)

```

__20. Aplique la validación cruzada simple para evaluar la robustez y capacidad predictiva del modelo.__
```{r}

library(caret)

cv <- trainControl(method = "cv", number = 10)
modelo_cruzado <- train(y_cuentas ~ x_distancia, tabla_datos, method = "lm", trControl = cv)

```


__21. Verifique que no existen observaciones influyentes.__
```{r}
influence.measures(modelo_entrenamiento)

```



__22. Verifique el supuesto de independencia de los residuos.__
```{r}
plot(modelo_entrenamiento)

```


__23. Confirme que los errores del modelo permanecen constantes para todo el rango de estimaciones.__
```{r}
acf(resid(modelo_entrenamiento))

```



